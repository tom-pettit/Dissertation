{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries required\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Ray COVID Positive: create the datasets \n",
    "\n",
    "class Covid_ChestXray_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 24])\n",
    "#         print(img_path)\n",
    "        with Image.open(img_path) as image:\n",
    "            label = self.img_labels.iloc[idx, 5]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# , transforms.ConvertImageDtype(torch.uint8)  \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "            \n",
    "            return image, label\n",
    "\n",
    "class HFlipped_Covid_ChestXray_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 24])\n",
    "#         print(img_path)\n",
    "        with Image.open(img_path) as image:\n",
    "            label = self.img_labels.iloc[idx, 5]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# , transforms.ConvertImageDtype(torch.uint8)  \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class VFlipped_Covid_ChestXray_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 24])\n",
    "#         print(img_path)\n",
    "        with Image.open(img_path) as image:\n",
    "            label = self.img_labels.iloc[idx, 5]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# , transforms.ConvertImageDtype(torch.uint8)   \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Rotated_Covid_ChestXray_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 24])\n",
    "#         print(img_path)\n",
    "        with Image.open(img_path) as image:\n",
    "            label = self.img_labels.iloc[idx, 5]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# , transforms.ConvertImageDtype(torch.uint8)  \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Translated_Covid_ChestXray_Dataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 24])\n",
    "#         print(img_path)\n",
    "        with Image.open(img_path) as image:\n",
    "            label = self.img_labels.iloc[idx, 5]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# specify the dataset path and the metadata\n",
    "metadata_path = 'dataset/covid-chestxray-dataset/processed_metadata.csv'\n",
    "images_path = 'dataset/covid-chestxray-dataset/images/'\n",
    "\n",
    "# create the datasets for the augmented images\n",
    "initial_covid_chestxray_dataset = Covid_ChestXray_Dataset(metadata_path, images_path)\n",
    "h_flipped_covid_chestxray_dataset = HFlipped_Covid_ChestXray_Dataset(metadata_path, images_path)\n",
    "v_flipped_covid_chestxray_dataset = VFlipped_Covid_ChestXray_Dataset(metadata_path, images_path)\n",
    "rotated_covid_chestxray_dataset = Rotated_Covid_ChestXray_Dataset(metadata_path, images_path)\n",
    "translated_covid_chestxray_dataset = Translated_Covid_ChestXray_Dataset(metadata_path, images_path)\n",
    "\n",
    "# combine the datasets\n",
    "combined_covid_chestxray_dataset = ConcatDataset([initial_covid_chestxray_dataset, h_flipped_covid_chestxray_dataset, v_flipped_covid_chestxray_dataset, rotated_covid_chestxray_dataset, translated_covid_chestxray_dataset])\n",
    "\n",
    "# randomly sample 2500 of these\n",
    "covid_chestxray_dataset = Subset(combined_covid_chestxray_dataset, np.random.choice(len(combined_covid_chestxray_dataset), 2500, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Ray Healthy Class: create datasets \n",
    "\n",
    "class healthy_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class healthy_HFlipped_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class healthy_VFlipped_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class healthy_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class healthy_HFlipped_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class healthy_VFlipped_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class healthy_Translated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class healthy_Translated_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class healthy_Perspective_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 0]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomPerspective(0.6, 1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "# specify the path for metadata and images\n",
    "metadata_path = 'processed_metadata.csv'\n",
    "images_path = 'dataset/chest-xray8/images'\n",
    "\n",
    "# create the datasets for the augmented images\n",
    "healthy_chest_xray8_dataset = healthy_Chest_Xray8(metadata_path, images_path)\n",
    "\n",
    "healthy_h_flipped_chest_xray8_dataset = healthy_HFlipped_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_v_flipped_chest_xray8_dataset = healthy_VFlipped_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_rotated_chest_xray8_dataset = healthy_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_h_flipped_rotated_chest_xray8_dataset = healthy_HFlipped_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_v_flipped_rotated_chest_xray8_dataset = healthy_VFlipped_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_translated_chest_xray8_dataset = healthy_Translated_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_translated_rotated_chest_xray8_dataset = healthy_Translated_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "healthy_perspective_chest_xray8_dataset = healthy_Perspective_Chest_Xray8(metadata_path, images_path)\n",
    "\n",
    "# combine all the augmented image datasets\n",
    "healthy_combined_chest_xray8_dataset = ConcatDataset([healthy_chest_xray8_dataset, healthy_h_flipped_chest_xray8_dataset, healthy_v_flipped_chest_xray8_dataset, healthy_rotated_chest_xray8_dataset, healthy_h_flipped_rotated_chest_xray8_dataset, healthy_v_flipped_rotated_chest_xray8_dataset, healthy_translated_chest_xray8_dataset, healthy_translated_rotated_chest_xray8_dataset, healthy_perspective_chest_xray8_dataset])\n",
    "\n",
    "# randomly subsample 2500 images\n",
    "healthy_chest_xray8_dataset = Subset(healthy_combined_chest_xray8_dataset, np.random.choice(len(healthy_combined_chest_xray8_dataset), 2500, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Ray Other Class: create datasets \n",
    "\n",
    "class other_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class other_HFlipped_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class other_VFlipped_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class other_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_HFlipped_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_VFlipped_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_Translated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_Translated_Rotated_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_Perspective_Chest_Xray8(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels['Finding Labels'] == 2]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomPerspective(0.6, 1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "# specify metadata and images path\n",
    "metadata_path = 'processed_metadata.csv'\n",
    "images_path = 'dataset/chest-xray8/images'\n",
    "\n",
    "# create the augmented image datasets\n",
    "other_chest_xray8_dataset = other_Chest_Xray8(metadata_path, images_path)\n",
    "\n",
    "other_h_flipped_chest_xray8_dataset = other_HFlipped_Chest_Xray8(metadata_path, images_path)\n",
    "other_v_flipped_chest_xray8_dataset = other_VFlipped_Chest_Xray8(metadata_path, images_path)\n",
    "other_rotated_chest_xray8_dataset = other_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "other_h_flipped_rotated_chest_xray8_dataset = other_HFlipped_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "other_v_flipped_rotated_chest_xray8_dataset = other_VFlipped_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "other_translated_chest_xray8_dataset = other_Translated_Chest_Xray8(metadata_path, images_path)\n",
    "other_translated_rotated_chest_xray8_dataset = other_Translated_Rotated_Chest_Xray8(metadata_path, images_path)\n",
    "other_perspective_chest_xray8_dataset = other_Perspective_Chest_Xray8(metadata_path, images_path)\n",
    "\n",
    "# combine the augmented images datasets\n",
    "other_combined_chest_xray8_dataset = ConcatDataset([other_chest_xray8_dataset, other_h_flipped_chest_xray8_dataset, other_v_flipped_chest_xray8_dataset, other_rotated_chest_xray8_dataset, other_h_flipped_rotated_chest_xray8_dataset, other_v_flipped_rotated_chest_xray8_dataset, other_translated_chest_xray8_dataset, other_translated_rotated_chest_xray8_dataset, other_perspective_chest_xray8_dataset])\n",
    "\n",
    "# subsample 2500 scans\n",
    "other_chest_xray8_dataset = Subset(other_combined_chest_xray8_dataset, np.random.choice(len(other_combined_chest_xray8_dataset), 2500, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT Scan Positive COVID-19 Class: create dataset\n",
    "\n",
    "class Sars_Cov_2_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = ['Covid ('+str(i)+').png' for i in range(1, 1251)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 1\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Rotated_Sars_Cov_2_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = ['Covid ('+str(i)+').png' for i in range(1, 1251)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 1\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "# specify the images path\n",
    "images_path = 'dataset/sars-cov-2-ct-scan/images/COVID/'\n",
    "\n",
    "# create the augmented image datasets\n",
    "Sars_Cov_2_CT_dataset = Sars_Cov_2_CT(images_path)\n",
    "rotated_Sars_Cov_2_CT_dataset = Rotated_Sars_Cov_2_CT(images_path)\n",
    "\n",
    "# combine the augmented datasets (2500 images exactly)\n",
    "Sars_Cov_2_CT_dataset = ConcatDataset([Sars_Cov_2_CT_dataset, rotated_Sars_Cov_2_CT_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT Scan Healthy Class: create the dataset\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "# specify the image path\n",
    "images_path = 'dataset/covid-ct/images/'\n",
    "class Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class HFlipped_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class VFlipped_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Rotated_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class HFlipped_rotated_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class VFlipped_rotated_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Translated_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Translated_Rotated_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomAffine(0, (0.2,0.2)),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class Perspective_Covid_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = [f for f in listdir(images_path)]\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 0\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomPerspective(0.6, 1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "# create the augmented image datasets\n",
    "covid_ct_dataset = Covid_CT(images_path)\n",
    "\n",
    "h_flipped_covid_ct_dataset = HFlipped_Covid_CT(images_path)\n",
    "v_flipped_covid_ct_dataset = VFlipped_Covid_CT(images_path)\n",
    "rotated_covid_ct_dataset = Rotated_Covid_CT(images_path)\n",
    "translated_covid_ct_dataset = Translated_Covid_CT(images_path)\n",
    "translated_rotated_covid_ct_dataset = Translated_Rotated_Covid_CT(images_path)\n",
    "h_flipped_rotated_covid_ct_dataset = HFlipped_rotated_Covid_CT(images_path)\n",
    "v_flipped_rotated_covid_ct_dataset = VFlipped_rotated_Covid_CT(images_path)\n",
    "perspective_covid_ct_dataset = Perspective_Covid_CT(images_path)\n",
    "\n",
    "# combine the augmented image datasets\n",
    "combined_covid_ct_dataset = ConcatDataset([covid_ct_dataset, h_flipped_covid_ct_dataset, v_flipped_covid_ct_dataset, rotated_covid_ct_dataset, h_flipped_rotated_covid_ct_dataset, v_flipped_rotated_covid_ct_dataset, translated_covid_ct_dataset, translated_rotated_covid_ct_dataset, perspective_covid_ct_dataset])\n",
    "\n",
    "# subsample 2500 datapoints\n",
    "covid_ct_dataset = Subset(combined_covid_ct_dataset, np.random.choice(len(combined_covid_ct_dataset), 2500, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT Scan Other Class: create dataset\n",
    "\n",
    "class other_Sars_Cov_2_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = os.listdir(img_dir)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_Rotated_Sars_Cov_2_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = os.listdir(img_dir)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "class other_HFlip_Sars_Cov_2_CT(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_labels = os.listdir(img_dir)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        with Image.open(img_path) as image:\n",
    "            label = 2\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # , transforms.ConvertImageDtype(torch.uint8)    \n",
    "            ])\n",
    "\n",
    "            image = transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "\n",
    "# specify the image path\n",
    "images_path = 'dataset/sars-cov-2-ct-scan/images/non-COVID/'\n",
    "\n",
    "# create the augmentged image datasets\n",
    "other_Sars_Cov_2_CT_dataset = other_Sars_Cov_2_CT(images_path)\n",
    "other_rotated_Sars_Cov_2_CT_dataset = other_Rotated_Sars_Cov_2_CT(images_path)\n",
    "other_HFlip_Sars_Cov_2_CT_dataset = other_HFlip_Sars_Cov_2_CT(images_path)\n",
    "\n",
    "# combine the augmented image datasets\n",
    "other_Sars_Cov_2_CT_dataset = ConcatDataset([other_Sars_Cov_2_CT_dataset, other_rotated_Sars_Cov_2_CT_dataset, other_HFlip_Sars_Cov_2_CT_dataset])\n",
    "\n",
    "# subsample 2500 datapoints\n",
    "other_Sars_Cov_2_CT_dataset = Subset(other_Sars_Cov_2_CT_dataset, np.random.choice(len(other_Sars_Cov_2_CT_dataset), 2500, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final dataloaders \n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# x ray dataloaders\n",
    "covid_xray_dataset = covid_chestxray_dataset\n",
    "healthy_xray_dataset = healthy_chest_xray8_dataset\n",
    "other_xray_dataset = other_chest_xray8_dataset\n",
    "\n",
    "# ct dataloaders\n",
    "covid_positive_ct_dataset = Sars_Cov_2_CT_dataset\n",
    "healthy_ct_dataset = covid_ct_dataset\n",
    "other_ct_dataset = other_Sars_Cov_2_CT_dataset\n",
    "\n",
    "# create combined datasets lists\n",
    "all_datasets = [covid_xray_dataset, healthy_xray_dataset, other_xray_dataset, covid_positive_ct_dataset, healthy_ct_dataset, other_ct_dataset]\n",
    "x_ray_datasets = [covid_xray_dataset, healthy_xray_dataset, other_xray_dataset]\n",
    "ct_datasets = [covid_positive_ct_dataset, healthy_ct_dataset, other_ct_dataset]\n",
    "\n",
    "# create combined datasets\n",
    "combined_dataset = ConcatDataset(all_datasets)\n",
    "x_ray_combined = ConcatDataset(x_ray_datasets)\n",
    "ct_combined = ConcatDataset(ct_datasets)\n",
    "\n",
    "# split combined dataset into train, validation and test splits\n",
    "train_size = int(0.7 * len(combined_dataset))\n",
    "val_size = int(0.2 * len(combined_dataset))\n",
    "test_size = int(0.1 * len(combined_dataset))\n",
    "train_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# create train, validation and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print('Total Dataset Size: ', len(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example batch from the dataloader\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "def show_databatch(inputs, classes):\n",
    "    out = make_grid(inputs)\n",
    "    imshow(out)\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(x_ray_train_healthy_or_not_dataloader))\n",
    "show_databatch(inputs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions: Training loop, testing loop, model evaluation, heatmap production\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, training_dataloader, validating_dataloader, training_dataset, validating_dataset, num_epochs=10, use_gpu=False):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss_val = 0\n",
    "    avg_acc_val = 0\n",
    "    \n",
    "    train_batches = len(training_dataloader)\n",
    "    val_batches = len(validating_dataloader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "        acc_train = 0\n",
    "        acc_val = 0\n",
    "        \n",
    "        model.train(True)\n",
    "        \n",
    "        for i, data in enumerate(training_dataloader):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
    "                \n",
    "            # Use half training dataset\n",
    "            if i >= train_batches / 2:\n",
    "                break\n",
    "                \n",
    "            inputs, labels = data\n",
    "            \n",
    "#             print(data)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.data\n",
    "            acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print()\n",
    "        # * 2 as we only used half of the dataset\n",
    "        avg_loss = loss_train * 2 / len(training_dataset)\n",
    "        avg_acc = acc_train * 2 / len(training_dataset)\n",
    "        \n",
    "        model.train(False)\n",
    "        model.eval()\n",
    "            \n",
    "        for i, data in enumerate(validating_dataloader):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
    "                \n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_val += loss.data\n",
    "            acc_val += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss_val = loss_val / len(validating_dataset)\n",
    "        avg_acc_val = acc_val / len(validating_dataset)\n",
    "        \n",
    "        print()\n",
    "        print(\"Epoch {} result: \".format(epoch))\n",
    "        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
    "        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
    "        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
    "        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
    "        print('-' * 10)\n",
    "        print()\n",
    "        \n",
    "        if avg_acc_val > best_acc:\n",
    "            best_acc = avg_acc_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def test_model(model, criterion, optimizer_ft, test_dataloader, test_dataset, use_gpu):\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "\n",
    "    test_labels = []\n",
    "    test_preds = []\n",
    "\n",
    "    test_batches = len(test_dataloader)\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\Test batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "        inputs, labels = data\n",
    "        \n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        optimizer_ft.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        all_labels = labels.data.cpu()\n",
    "        all_preds = preds.cpu()\n",
    "        for i in range(len(labels.data.cpu())):\n",
    "            test_labels.append(all_labels[i])\n",
    "            test_preds.append(all_preds[i])\n",
    "\n",
    "        loss_test += loss.data\n",
    "        acc_test += torch.sum(preds == labels.data)\n",
    "\n",
    "        del inputs, labels, outputs, preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss_test = loss_test / len(test_dataset)\n",
    "    avg_acc_test = acc_test / len(test_dataset)\n",
    "\n",
    "    print()\n",
    "    print(\"Avg loss (test): {:.4f}\".format(avg_loss_test))\n",
    "    print(\"Avg acc (test): {:.4f}\".format(avg_acc_test))\n",
    "    print('-' * 10)\n",
    "    print()\n",
    "    \n",
    "    return test_labels, test_preds\n",
    "    \n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# produce confusion matrix and classification report\n",
    "def evaluate_model(test_labels, test_preds):\n",
    "    conf_matrix = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,\n",
    "                                  display_labels=['healthy', 'covid', 'other'])\n",
    "\n",
    "    classif_report = classification_report(test_labels, test_preds)\n",
    "    print(classif_report)\n",
    "\n",
    "    print(disp.plot())\n",
    "    print(conf_matrix)\n",
    "\n",
    "    RocCurveDisplay.from_predictions(test_labels, test_preds, pos_label=1)\n",
    "    \n",
    "# produce a heatmap for the given model on a sample from the dataloader\n",
    "def show_heatmap(model, test_dataloader, vgg16=False, densenet=False, darknet=False, efficientnet=False):    \n",
    "    if vgg16 or densenet:\n",
    "        target_layer = model.features[-1]\n",
    "    elif darknet:\n",
    "        target_layer = model.conv5[-4]\n",
    "    elif efficientnet:\n",
    "        target_layer = model.features[-1]\n",
    "        \n",
    "    # ensure its a heatmap of a COVID positive image\n",
    "    index = 0\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            input_tensor = inputs[i,:]\n",
    "            label = np.array(labels[i])\n",
    "            if label == 1:\n",
    "                index = i\n",
    "\n",
    "        break \n",
    "\n",
    "    input_tensor = inputs[index,:]\n",
    "    label = np.array(labels[index])\n",
    "    input_tensor = torch.tensor(np.expand_dims(input_tensor, axis=0))\n",
    "\n",
    "    from pytorch_grad_cam import GradCAM\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=True)\n",
    "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "    targets = [ClassifierOutputTarget(1)]\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    \n",
    "    # ROAD Most Relevant First calculation \n",
    "    from pytorch_grad_cam.metrics.road import ROADMostRelevantFirst, ROADLeastRelevantFirst\n",
    "    # alter the percentile argument to test at different percentiles of pixel importances\n",
    "    cam_metric = ROADMostRelevantFirst(percentile=90)\n",
    "    scores, perturbation_visualizations = cam_metric(input_tensor.cuda(), \n",
    "    grayscale_cam, targets, model, return_visualization=True)\n",
    "\n",
    "    # specify the metric and the percentiles for ROAD Combined\n",
    "    from pytorch_grad_cam.metrics.road import ROADCombined\n",
    "    cam_metric = ROADCombined(percentiles=[50, 60, 70, 80, 90])\n",
    "    road_combined_scores = cam_metric(input_tensor.cuda(), grayscale_cam, targets, model)\n",
    "    print(f\"Combined metric avg confidence increase with ROAD accross 5 thresholds (positive is better): {road_combined_scores[0]}\")\n",
    "\n",
    "    # produce the image after pixels perturbated\n",
    "    perturbation_visualizations = (perturbation_visualizations+1)*0.5\n",
    "    perturbation_visualizations = (np.array(perturbation_visualizations[0].cpu() * 255)).astype(np.uint8)\n",
    "    perturbation_visualizations = perturbation_visualizations.transpose((1, 2, 0))\n",
    "    evaluation_out = Image.fromarray(perturbation_visualizations)\n",
    "\n",
    "    input_copy = input_tensor.detach().clone().cuda()\n",
    "    output = model(input_copy)\n",
    "    _, prediction = torch.max(output.data, 1)\n",
    "\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "    import cv2 \n",
    "\n",
    "    # overlay the heatmap onto the original image\n",
    "    new_input_tensor = input_tensor[0,:]\n",
    "\n",
    "    img = np.array(new_input_tensor)\n",
    "\n",
    "    new_img = img.transpose((1, 2, 0))\n",
    "\n",
    "    new_img = (new_img+1)*0.5\n",
    "\n",
    "    new_img = np.uint8(255*new_img)\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                ])\n",
    "\n",
    "    new_img = np.array(transform(new_img)) \n",
    "\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_HOT)\n",
    "\n",
    "    # heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_weight = 0.7\n",
    "\n",
    "    cam = (1 - image_weight) * heatmap + image_weight * new_img\n",
    "\n",
    "    cam = np.uint8(cam)\n",
    "\n",
    "    out = Image.fromarray(np.hstack((new_img, heatmap, cam)))\n",
    "    \n",
    "    return out, label, prediction, scores, evaluation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "vgg16 = vgg16()\n",
    "num_features = vgg16.classifier[6].in_features\n",
    "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 3)]) # Change output size to 3\n",
    "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "vgg16.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# vgg16 = train_model(vgg16, criterion, optimizer_ft, train_dataloader, val_dataloader, train_dataset, val_dataset, num_epochs=100, use_gpu=use_gpu)\n",
    "# torch.save(vgg16.state_dict(), './trained_models/VGG16/VGG16.pt')\n",
    "vgg16.load_state_dict(torch.load('./trained_models/VGG16/VGG16.pt'))\n",
    "\n",
    "import time \n",
    "\n",
    "# evaluate the trained model on the test set\n",
    "start = time.time()\n",
    "test_labels, test_preds = test_model(vgg16, criterion, optimizer_ft, test_dataloader, test_dataset, True)\n",
    "end = time.time()\n",
    "\n",
    "# produce confusion matrix and classification report\n",
    "evaluate_model(test_labels, test_preds)\n",
    "\n",
    "print('time taken: ', end - start)\n",
    "\n",
    "# output the perturbated image and ROAD scores\n",
    "heatmap, label, prediction, scores, perturbation_visualizations = show_heatmap(vgg16, test_dataloader, vgg16=True)\n",
    "print(scores)\n",
    "print(perturbation_visualizations)\n",
    "file = './trained_models/VGG16/evaluation_heatmap.jpg'\n",
    "perturbation_visualizations.save(file)\n",
    "\n",
    "# save the heatmap\n",
    "file = './trained_models/VGG16/heatmap.jpg'\n",
    "heatmap.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import densenet201\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "densenet = densenet201()\n",
    "num_features = densenet.classifier.in_features\n",
    "features = list(densenet.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 3)]) # Change output size to 3\n",
    "densenet.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "densenet.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(densenet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# densenet = train_model(densenet, criterion, optimizer_ft, train_dataloader, val_dataloader, train_dataset, val_dataset, num_epochs=100, use_gpu=use_gpu)\n",
    "# torch.save(densenet.state_dict(), './trained_models/DenseNet201/DenseNet201.pt')\n",
    "densenet.load_state_dict(torch.load('./trained_models/DenseNet201/DenseNet201.pt'))\n",
    "\n",
    "import time \n",
    "\n",
    "# evaluate the trained model on the test set\n",
    "start = time.time()\n",
    "test_labels, test_preds = test_model(densenet, criterion, optimizer_ft, test_dataloader, test_dataset, True)\n",
    "end = time.time()\n",
    "\n",
    "# produce the confusion matrix and classification report\n",
    "evaluate_model(test_labels, test_preds)\n",
    "print('time taken: ', end - start)\n",
    "\n",
    "# output the perturbated image and ROAD scores\n",
    "heatmap, label, prediction, scores, perturbation_visualizations = show_heatmap(densenet, test_dataloader, densenet=True)\n",
    "print(scores)\n",
    "print(perturbation_visualizations)\n",
    "file = './trained_models/DenseNet201/evaluation_heatmap.jpg'\n",
    "perturbation_visualizations.save(file)\n",
    "\n",
    "# save the heatmap\n",
    "file = './trained_models/DenseNet201/heatmap.jpg'\n",
    "heatmap.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DarkNet19\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Specify Global Avg Pooling layer required for DarkNet19\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.data.size(0)\n",
    "        C = x.data.size(1)\n",
    "        H = x.data.size(2)\n",
    "        W = x.data.size(3)\n",
    "        \n",
    "        x = nn.functional.avg_pool2d(x, (H,W))\n",
    "        x = x.view(N, C)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# The DarkNet19 architecture\n",
    "class DarkNet19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DarkNet19, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.conv3 = nn.Sequential(    \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.conv5 = nn.Sequential(    \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(    \n",
    "            nn.Conv2d(1024, 3, kernel_size=(1,1), stride=(1,1)),\n",
    "            GlobalAvgPool2d(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "darknet = DarkNet19()\n",
    "\n",
    "darknet.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(darknet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# darknet = train_model(darknet, criterion, optimizer_ft, train_dataloader, val_dataloader, train_dataset, val_dataset, num_epochs=100, use_gpu=use_gpu)\n",
    "# torch.save(darknet.state_dict(), './trained_models/DarkNet19/DarkNet19.pt')\n",
    "darknet.load_state_dict(torch.load('./trained_models/DarkNet19/DarkNet19.pt'))\n",
    "\n",
    "import time \n",
    "\n",
    "# evaluate the trained model on the test set\n",
    "start = time.time()\n",
    "test_labels, test_preds = test_model(darknet, criterion, optimizer_ft, test_dataloader, test_dataset, True)\n",
    "end = time.time()\n",
    "\n",
    "# produce the confusion matrix and classification report\n",
    "evaluate_model(test_labels, test_preds)\n",
    "print('time taken: ', end - start)\n",
    "\n",
    "# output the perturbated image and ROAD scores\n",
    "heatmap, label, prediction, scores, perturbation_visualizations = show_heatmap(darknet, test_dataloader, darknet=True)\n",
    "print(scores)\n",
    "print('Label here: ', label)\n",
    "print(perturbation_visualizations)\n",
    "file = './trained_models/DarkNet19/evaluation_heatmap.jpg'\n",
    "perturbation_visualizations.save(file)\n",
    "\n",
    "# save the heatmap\n",
    "file = './trained_models/DarkNet19/heatmap.jpg'\n",
    "heatmap.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "efficientnet = efficientnet_b0()\n",
    "num_features = efficientnet.classifier[1].in_features\n",
    "features = list(efficientnet.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 3)]) # Change output size to 3\n",
    "efficientnet.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "efficientnet.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(efficientnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# efficientnet = train_model(efficientnet, criterion, optimizer_ft, train_dataloader, val_dataloader, train_dataset, val_dataset, num_epochs=100, use_gpu=use_gpu)\n",
    "# torch.save(efficientnet.state_dict(), './trained_models/EfficientNetB0/EfficientNetB0.pt')\n",
    "efficientnet.load_state_dict(torch.load('./trained_models/EfficientNetB0/EfficientNetB0.pt'))\n",
    "\n",
    "import time \n",
    "\n",
    "# evaluate the trained model on the test set\n",
    "start = time.time()\n",
    "test_labels, test_preds = test_model(efficientnet, criterion, optimizer_ft, test_dataloader, test_dataset, True)\n",
    "end = time.time()\n",
    "\n",
    "# produce the confusion matrix and classification report\n",
    "evaluate_model(test_labels, test_preds)\n",
    "print('time taken: ', end - start)\n",
    "\n",
    "# produce the perturbated image and ROAD scores\n",
    "heatmap, label, prediction, scores, perturbation_visualizations = show_heatmap(efficientnet, test_dataloader, efficientnet=True)\n",
    "print(scores)\n",
    "print(perturbation_visualizations)\n",
    "file = './trained_models/EfficientNetB0/evaluation_heatmap.jpg'\n",
    "perturbation_visualizations.save(file)\n",
    "\n",
    "# save the heatmap \n",
    "file = './trained_models/EfficientNetB0/heatmap.jpg'\n",
    "heatmap.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed Ensemble Model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "# The VGG16 model\n",
    "\n",
    "vgg16 = vgg16()\n",
    "num_features = vgg16.classifier[6].in_features\n",
    "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 3)]) # Change output size to 3\n",
    "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "vgg16.load_state_dict(torch.load('./trained_models/VGG16/VGG16.pt'))\n",
    "\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "\n",
    "# The EfficientNetB0 model\n",
    "\n",
    "efficientnet = efficientnet_b0()\n",
    "num_features = efficientnet.classifier[1].in_features\n",
    "features = list(efficientnet.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 3)]) # Change output size to 3\n",
    "efficientnet.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "efficientnet.cuda()\n",
    "\n",
    "efficientnet.load_state_dict(torch.load('./trained_models/EfficientNetB0/EfficientNetB0.pt'))\n",
    "\n",
    "# Define the ensemble model structure\n",
    "class MyEnsemble(nn.Module):\n",
    "\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.modelA(x)\n",
    "        out2 = self.modelB(x)\n",
    "\n",
    "        out = out1 + out2\n",
    "\n",
    "        return self.fc1(out)\n",
    "\n",
    "ensemblenet = MyEnsemble(vgg16, efficientnet)\n",
    "ensemblenet.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(ensemblenet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# ensemblenet = train_model(ensemblenet, criterion, optimizer_ft, train_dataloader, val_dataloader, train_dataset, val_dataset, num_epochs=100, use_gpu=use_gpu)\n",
    "# torch.save(ensemblenet.state_dict(), './latest_ensemble_method/ensemblenet.pt')\n",
    "ensemblenet.load_state_dict(torch.load('./latest_ensemble_method/ensemblenet.pt'))\n",
    "\n",
    "import time \n",
    "\n",
    "# evaluate the trained model on the test set\n",
    "start = time.time()\n",
    "test_labels, test_preds = test_model(ensemblenet, criterion, optimizer_ft, test_dataloader, test_dataset, True)\n",
    "end = time.time()\n",
    "\n",
    "# produce the confusion matrix and classification report\n",
    "evaluate_model(test_labels, test_preds)\n",
    "\n",
    "print('time taken: ', end - start)\n",
    "\n",
    "# produce the perturbated image and ROAD scores\n",
    "heatmap, label, prediction, scores, perturbation_visualizations = show_heatmap(ensemblenet, test_dataloader, ensemblenet=True)\n",
    "print('scores: ', scores)\n",
    "print('Label here: ', label)\n",
    "print(perturbation_visualizations)\n",
    "file = './latest_ensemble_method/evaluation_heatmap.jpg'\n",
    "perturbation_visualizations.save(file)\n",
    "\n",
    "# save the heatmap\n",
    "file = './latest_ensemble_method/heatmap.jpg'\n",
    "heatmap.save(file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
